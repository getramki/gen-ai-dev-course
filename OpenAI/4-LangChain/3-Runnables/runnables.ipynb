{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Runnables Work in LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "In LangChain, a runnable can be any Python callable, such as a function, a lambda expression, or an instance method of a class. However, instead of directly passing these callables around, you wrap them in a runnable object to provide additional functionality and metadata, like the function name, execution time, or custom annotations.\n",
    "\n",
    "Here's an example of how you can create a runnable from a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Rama!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Define a simple function\n",
    "def greet(name):\n",
    "   return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "# Wrap the function in a RunnableWrapper\n",
    "greet_runnable = RunnableLambda(lambda x: greet(x))\n",
    "\n",
    "\n",
    "# Use the runnable to call the function\n",
    "result = greet_runnable.invoke(\"Rama\")\n",
    "print(result)  # Output: Hello, Rama!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we defined a simple `greet` function that takes a name as an argument and returns a greeting string. This function is then wrapped in a `RunnableWrapper`. \n",
    "\n",
    "`greet_runnable` provides additional functionality and metadata, making it easier to integrate with other parts of your code. This allows you to manage and pass around multiple callables with additional context or behavior. \n",
    "\n",
    "One advantage of wrapping callables as runnables is you can now connect them using LangChain's chaining mechanisms, such as the pipe operator (`|`), the `RunnableSequence` class, or the . pipe( ) method.\n",
    "\n",
    "For example, you can use `RunnableSequence` to create a chain applying multiple transformations to some input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO, RAMA! THE CURRENT DATE AND TIME IS 2024-09-14 15:53:45.!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "# Define the transformations as simple functions\n",
    "def greet(name):\n",
    "   return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "def append_datetime(text):\n",
    "   current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "   return f\"{text} The current date and time is {current_datetime}.\"\n",
    "\n",
    "\n",
    "def to_uppercase(text):\n",
    "   return text.upper()\n",
    "\n",
    "\n",
    "def add_exclamation(text):\n",
    "   return f\"{text}!\"\n",
    "\n",
    "\n",
    "# Wrap the functions in RunnableWrapper\n",
    "greet_runnable = RunnableLambda(lambda x: greet(x))\n",
    "datetime_runnable = RunnableLambda(lambda x: append_datetime(x))\n",
    "uppercase_runnable = RunnableLambda(lambda x: to_uppercase(x))\n",
    "exclamation_runnable = RunnableLambda(lambda x: add_exclamation(x))\n",
    "\n",
    "\n",
    "# Create a RunnableSequence with the wrapped runnables\n",
    "chain = RunnableSequence(\n",
    "   first=greet_runnable,\n",
    "   middle=[datetime_runnable, uppercase_runnable],\n",
    "   last=exclamation_runnable,\n",
    ")\n",
    "\n",
    "\n",
    "# Apply the chain to some input data\n",
    "input_data = \"Rama\"\n",
    "result = chain.invoke(input_data)\n",
    "print(\n",
    "   result\n",
    ")  # Output example: \"HELLO, ALICE! THE CURRENT DATE AND TIME IS 2024-06-19 14:30:00!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have four simple functions: `greet`, `append_datetime`, `to_uppercase`, and `add_exclamation`, each of which takes input and performs a specific transformation on it. `RunnableLambda` takes a function as its argument, and creates a runnable object.\n",
    "\n",
    "We can then create a `RunnableSequence` by passing these runnables to its constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(\n",
    "   first=greet_runnable,\n",
    "   middle=[datetime_runnable, uppercase_runnable],\n",
    "   last=exclamation_runnable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableSequence` executes these runnables in sequential order, using the output of one runnable as input to the next.\n",
    "\n",
    "The result of a chain is a `RunnableSequence` which is still a runnable that can still be piped, invoked, streamed, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Runnable with the Chain Decorator\n",
    "The `@chain` decorator allows you to turn any function into a chain. Below, the decorator creates a custom chain that combines multiple components, such as prompts, models, and output parsers, and defines a function (`custom_chain`) that encapsulates the sequence of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = ChatOpenAI().invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n",
    "    return chain2.invoke({\"joke\": parsed_output1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`invoke`, `batch`, and `stream` Methods\n",
    "As previously mentioned, LangChain runnables provide three key methods to execute and interact with your chains:\n",
    "\n",
    "`invoke`: executes a runnable with a single input, and is typically used when you have a single piece of data to process.\n",
    "‍\n",
    "`batch`: allows you to process multiple inputs in parallel. This method is useful when you have a list of inputs and want to run them through the chain simultaneously.\n",
    "‍\n",
    "`stream`: processes input data as a stream, handling one piece of data at a time and providing results as they are available. This method is ideal for handling streamed output for real-time data processing or for large datasets that you want to process incrementally. At the time of this writing, streaming support for retries is being added for higher reliability without any latency cost (as explained in their docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Runnable Types in LangChain\n",
    "\n",
    "Within LangChain, you have access to various runnable types that allow you to execute and manage tasks:\n",
    "\n",
    "`RunnableParallel` for parallelizing operations.\n",
    "‍\n",
    "`RunnablePassthrough` for passing data unchanged from previous steps for use as input in later steps.    \n",
    "‍\n",
    "`RunnableLambda` for converting a Python callable into a runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "This runs a mapping of runnables in parallel and returns a mapping of their outputs. It’s essentially a dictionary whose values are runnables, and it invokes them concurrently, providing the same input to each. \n",
    "\n",
    "A `RunnableParallel` can be instantiated directly or by using a dictionary literal within a sequence. This is particularly useful when you want to parallelize operations or manipulate the output of one runnable to match the input format of the next runnable in a sequence.\n",
    "\n",
    "Below is an example that uses functions to illustrate how `RunnableParallel` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mul_two': 4, 'mul_three': 6}\n",
      "[{'mul_two': 4, 'mul_three': 6}, {'mul_two': 6, 'mul_three': 9}, {'mul_two': 8, 'mul_three': 12}]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "   return x + 1\n",
    "\n",
    "def mul_two(x: int) -> int:\n",
    "   return x * 2\n",
    "\n",
    "def mul_three(x: int) -> int:\n",
    "   return x * 3\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(mul_two)\n",
    "runnable_3 = RunnableLambda(mul_three)\n",
    "\n",
    "\n",
    "sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n",
    "   \"mul_two\": runnable_2,\n",
    "   \"mul_three\": runnable_3,\n",
    "}\n",
    "# Or equivalently:\n",
    "# sequence = runnable_1 | RunnableParallel(\n",
    "#     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n",
    "# )\n",
    "# Also equivalently:\n",
    "# sequence = runnable_1 | RunnableParallel(\n",
    "#     mul_two=runnable_2,\n",
    "#     mul_three=runnable_3,\n",
    "# )\n",
    "\n",
    "\n",
    "print(sequence.invoke(1))\n",
    "# > {'mul_two': 4, 'mul_three': 6}\n",
    "print(sequence.batch([1, 2, 3]))\n",
    "# > [{'mul_two': 4, 'mul_three': 6}, {'mul_two': 6, 'mul_three': 9}, {'mul_two': 8, 'mul_three': 12}]\n",
    "\n",
    "\n",
    "async def async_invoke(sequence, x):\n",
    "   return await sequence.ainvoke(x)\n",
    "\n",
    "\n",
    "async def async_batch(sequence, x):\n",
    "   return await sequence.abatch(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "This is a runnable that passes inputs through unchanged or with additional keys. It behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dictionary. \n",
    "\n",
    "It’s often used in conjunction with `RunnableParallel` to pass data through to a new key in the map, which allows you to keep the original input intact while adding some extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `passed` key was called with `RunnablePassthrough`, passing on the input data {'num': 1} without changes. And, the modified key was set using a lambda that added 1 to 'num', resulting in `modified` having the value `2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "`RunnableLambda` is a LangChain abstraction that allows Python-callable functions to be transformed into functions compatible with LangChain's pipeline operations. \n",
    "\n",
    "Wrapping a callable in a `RunnableLambda` makes the callable usable within either a `sync` or `async` context and can be composed as any other runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a RunnableLambda\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "runnable = RunnableLambda(add_one)\n",
    "\n",
    "runnable.invoke(1) # returns 2\n",
    "runnable.batch([1, 2, 3]) # returns [2, 3, 4]\n",
    "\n",
    "# Async is supported by default by delegating to the sync implementation\n",
    "await runnable.ainvoke(1) # returns 2\n",
    "await runnable.abatch([1, 2, 3]) # returns [2, 3, 4]\n",
    "\n",
    "\n",
    "# Alternatively, can provide both synd and sync implementations\n",
    "async def add_one_async(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "runnable = RunnableLambda(add_one, afunc=add_one_async)\n",
    "runnable.invoke(1) # Uses add_one\n",
    "await runnable.ainvoke(1) # Uses add_one_async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the code handles individual values and batches of data, using the provided `sync` and `async` implementations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
