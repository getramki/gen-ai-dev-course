{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "#we will create 5 datapoints to evaluate on. We will be evaluating a question-answering \n",
    "# application. The input will be a question, and the output will be an answer.\n",
    "#  Since this is a question-answering application, we can define the expected answer. \n",
    "#Let's see how to create and upload this dataset to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"What is LangChain?\"},\n",
    "        {\"question\": \"What is LangSmith?\"},\n",
    "        {\"question\": \"What is OpenAI?\"},\n",
    "        {\"question\": \"What is Google?\"},\n",
    "        {\"question\": \"What is Mistral?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"A framework for building LLM applications\"},\n",
    "        {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        {\"answer\": \"A technology company known for search\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page, \n",
    "# when we click into it we should see that\n",
    "#  we have five new examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Metrics <br>\n",
    "For the first, we will use an LLM to judge whether the output is correct <br> (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too<br >complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "eval_llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def evaluate_length(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"answer\") or \"\"\n",
    "    score = int(len(prediction) < 2 * len(required))\n",
    "    return {\"key\":\"length\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run Evaluations</h1>\n",
    "Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langsmith_app(inputs):\n",
    "    output = my_app(inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Great! Now we're ready to run evaluation. Let's do it! \n",
    "# Click on link to see evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmalhariwa001\\Desktop\\Gen AI\\LangChain\\Langchain-Detailed\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-3.5-5c2d8981' at:\n",
      "https://smith.langchain.com/o/36dd4cf9-88ca-5121-8019-46bec09db125/datasets/c9d53d0c-81f3-4e08-b803-d3f5ab1f8326/compare?selectedSessions=1c9f2e6f-d2ce-4130-8211-284c0f53d125\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:08,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-4-5e0210d8' at:\n",
      "https://smith.langchain.com/o/36dd4cf9-88ca-5121-8019-46bec09db125/datasets/c9d53d0c-81f3-4e08-b803-d3f5ab1f8326/compare?selectedSessions=2e871dff-768e-4cac-a1a1-3f24c4f5bd03\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:11,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Let's now try it out with a different model! Let's try gpt-4-turbo\n",
    "\n",
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app_1(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "def langsmith_app_1(inputs):\n",
    "    output = my_app_1(inputs[\"question\"])\n",
    "    return {\"output\": output}\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app_1, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"openai-4\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##And now let's use GPT-4 but \n",
    "# also update the prompt to be a bit more strict in requiring the answer to be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'strict-openai-4-ff0f0388' at:\n",
      "https://smith.langchain.com/o/36dd4cf9-88ca-5121-8019-46bec09db125/datasets/c9d53d0c-81f3-4e08-b803-d3f5ab1f8326/compare?selectedSessions=e6266846-9da6-4b68-94e4-f3551423c07b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app_2(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "def langsmith_app_2(inputs):\n",
    "    output = my_app_2(inputs[\"question\"])\n",
    "    return {\"output\": output}\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app_2, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"strict-openai-4\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Set up automated testing to run in CI/CD</h1>\n",
    "Now that we've run this in a one-off manner, we can set it to run in an automated fashion.\n",
    "We can do this pretty easily by just including it as a pytest file that we run in CI/CD.\n",
    "As part of this, we can either just log the results OR set up some criteria to determine \n",
    "if it passes or not. For example, if I wanted to ensure that we always got at least 80% of \n",
    "generated responses passing the length check, we could set that up with a test like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_length_score() -> None:\n",
    "    \"\"\"Test that the length score is at least 80%.\"\"\"\n",
    "    experiment_results = evaluate(\n",
    "        langsmith_app, # Your AI system\n",
    "        data=dataset_name, # The data to predict and grade over\n",
    "        evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    )\n",
    "    # This will be cleaned up in the next release:\n",
    "    feedback = client.list_feedback(\n",
    "        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\n",
    "        feedback_key=\"length\"\n",
    "    )\n",
    "    scores = [f.score for f in feedback]\n",
    "    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
